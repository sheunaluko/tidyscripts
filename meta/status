
Previous Work Summary:
- Implemented cortex_0 voice agent with LLM integration, sandbox execution, and 12-widget UI (see apps/ts_next_app/app/laboratory/cortex_0)
- Built comprehensive architecture documentation (cortex_architecture_information.md) covering agent loop, sandbox isolation, widget system, and execution history
- Created InsightsClient for event tracking with batching, event chains, and SurrealDB persistence
- Developed Meditation app as reference implementation for InsightsClient integration
- Added insights_agent_guide.md (2,697 lines) for AI agent integration patterns


Todo

[ ] SmartChats MVP
   AUDIO 
  - [x] Add a debug panel showing Voice Activity Detection (VAD) parameters (the internals to udnerstand triggering) 
  - [x] Improve start/stop behavior: auto-listening mode unless the system is speaking then default to VAD 
  - [x] Implement debouncing to merge or cancel rapid back-to-back inputs


   DISPLAY
  - [x] DEFERRED - fix the display of function calls (the react object inspector) -- just needs to DEFAULT TO BE CLOSED
  - [ ] Default execution history to horizontal 

   SYSTEM 
  - [ ] Update the declarative knowledge base
        - have built in relations + a misc  OR
	- just put relation type inside metadata on relation (not ideal for complex queries) 
	
  - [ ] Refine the initialization sequence and return-handling logic
     - [ ] Obtain clear examples from the Insights API
  - [x] token counting
  - [ ] parse errors in code execution to determine the needed javascript globals (Array, Date, etc) 
  - [ ] update model selector to reflect model registry
  - [ ] do not allow model or audio settings changes while running 

   FREEDOM
  - [ ] Add a paywall and deploy on SmartChats.AI - target date -> mid february (valentines day) 
   




[x] Observability - Phase 1 Complete
   [x] InsightsClient (Write-side: event capture with batching)
   [x] Meditation App (Test implementation + reference)
   [x] Insights API documentation (insights_agent_guide.md)
   [x] Integrate InsightsClient into cortex_0 (with LLM/execution tracking)
   [~] ReflectionsClient (Read-side: query and analytics) - IN PROGRESS
       [x] Phase 1: Foundation (Week 1) - COMPLETED
           [x] Created type definitions (types/reflections.ts)
               - QueryFilter, QueryResult, SessionAnalysis, TraceAnalysis
               - LLMAnalytics, PerformanceMetrics, ErrorAnalysis
               - ReflectionsConfig with cache/threshold settings
           [x] Implemented ReflectionsClient class (apis/reflections.ts)
               - queryEvents() with cache integration (MemoryCache, 60s TTL)
               - Convenience methods: getEventsBySession, getEventsByTrace, getEventsByType
               - Factory pattern: createClient, getDefaultClient, setDefaultClient
               - Silent failure mode, enable/disable, cache invalidation
               - Stubs for Phase 3/4 methods (analyzeSession, analyzeLLMUsage, etc)
           [x] Updated package exports (apis/index.ts, index.ts)
           [x] Created test suite (reflections.test.ts)
           [x] Created usage examples (reflections_example.ts)
       [ ] Phase 2: Query Enhancement (Week 2) - NEXT
           [ ] Create /api/insights/query_advanced endpoint
               - Advanced filters: tags (OR/AND), time range, duration
               - Pagination and sorting support
               - Dynamic SurrealQL query building
           [ ] Update ReflectionsClient to use advanced endpoint
           [ ] Test complex multi-filter queries
       [ ] Phase 3: Analysis Operations (Week 3)
           [ ] Implement analyzeSession() - session metrics and statistics
           [ ] Implement analyzeTrace() - event chain analysis with depth calculation
           [ ] Add event chain tree building utilities
       [ ] Phase 4: LLM & Performance Analytics (Week 4)
           [ ] Implement analyzeLLMUsage() - token usage, costs, latency percentiles
           [ ] Implement analyzePerformance() - slow operation detection, percentiles
           [ ] Implement analyzeErrors() - error pattern analysis 

   [ ] Arch
     --> Built using Insights/Reflections 
     --> realized that defining the enitre shape of the application from event based modeling of user interaction with built in telemetry to database storage and verification		of telemetry data using SIMULATED user interaction event streams.
     	 This is the target. Either requires playwright OR -- an architecture that is runnable on server side
     --> workflow: AI doesnt not write any code -- instead models the UI events that occur , the app states , key function INPUTS/OUTPUTS
     	 	   - ARCH is a system for specifying all of this and actually "type checking" it with dummy data and dummy events , all the way to generating
		     "fake" UI flows (actually reflected in the UI and telemetry data on a "fake") backend
	      - ARCH utilizes PARAMETERS which are included as part of the telemetry data
	      	  - a parameter is ANYTHING WHICH CAN BE OPTIMIZED VIA THE arch feedback loop
		  - also a parameter should persist despite changes to the code implementation (unless even particular implementations are
		    considered different parameters, which would allow the AI to optimizing the
		    actual CODE for the application within the loop as well -- which seems necessary actually) 
		  - the [loop is closed] when analysis on the telemetry workflow data leads to proposed parameter changes (this could even include
		    prompts, configs, etc, ?Code) , and then re-running the app



[ ] got lotus working -- but now need to build a proper UI for it to debug it
    	- [ ] need to build token manager (OR -- need to have automatic storage to workspace ) 
	
[x] dolphin parser working 4 bit 


[ ] Web interface where I can remotely control claude sessions 

[ ] ability to store an entire session and then re-load it, or switch llm
providers in middle




[ ] user scoped RELATION architecture -- ?

[ ] define metrics on the page:
     - a measure of how fluiid the voice interactioin is updating  (simple FPS metric using performance.now and reauqestAnimationFrame)
     - how much RAM?  heap/etc using performance
     - token usage high jumps in token usage suggesting opportunity for optimizing context
     - LLM latency 

[ ] function call RETURN values polluting UI with embedding??
[ ] need to discourage arbitrary logging to preserve context space
[ ] ability to cancel running LLM -- and to send alternative message.
  - implementation - the sandbox execution awaits Promise.all[ exec , user_cancel ]
[ ] Fix the bug where it hears speech and sends it before im done speaking, then sends the rest (two calls)
  before cortex has responded 


[ ] add delay from when the VAD cancels speech detection to when listening is triggered , so the user says STOP , verifies pause, then states query
[ ] improve the ability to detect the first couple of words in SR 

[ ] function to reset cortex state/context
[ ] download model information and create a feedback of context percent and cost based on token count

[ ] unable to resize grid widget to 1 width

[ ] make the HTML window aware of dark mode vs light mode , or just generate theme neutral content
  - [ ] i want to think more about the HTML widget , since it is proving to be WAAAAAY more impressive and useful than i thought it would be.
  - [ ] this is a signal that i want to tune into 

[ ] 3d projection of vector embedding results -- can cortex use the ui libs? 
[ ] create guards that automatically prevent context from getting overloadded

[ ] console.log is taking up token usage


Sandbox Errors
 - [] parseInt not a function
 - [] Array.isArray not a function
 - hmmm ? how to approach this robust 



[x] test the initialization flow
[x] persist dark/light mode choice to LS  
[x] bootstrap call chains that exist -> this was a very exciting and interesting process! lol wish I had recorded it
[x] still trying to fix the dynamic function calls -- the architecture is overly complex at this point
due to membrane funcitonality. Although it seems i am close to have it working without errors im getting nothing save to db although it looks like something is saved to db --- confusing! 
    -> this bug was due to the access db function itself which had conflicting documentation and was expecting variables to be merged into params as apposed to {variables, query} 
[x] implemented iframe executation with configurable contex
[x] re-architect cortex to use the JS exec env (sandbox.ts)
[x] fix the get_user_input function  (also should fire event? to be displayed in UI
[x] fix the return values of the functions (dont need the @ pattern from the embedding computation anymore
[x] improve the thoughts UI with a brain emoji to start the line of each thought
[x] dynamic function calling needs to be more debuggable - for example the run_dfn needs
to actually RETURN a result as well as an error etc -- right now just seeing nothing,
including no ersult in the database 
[x] add widget that enables paging through JS exec history (see prior code, fn calls)
[x] default speed 1.5

--- 

[x] when function returns, should ?CAPTURE its value and append it to the function call U
[x] need to rearchitect the call chain templates system  (need to bootstrap the


[ ] create voice interface to claude code 
[ ] Also check out PLAYRIGHT MCP server 






== CORTEX == 
[x] Implemented matrix
[x] Need to integrate matrix with cortex
[x] -> completed the integration, and upgraded the responses api. however I am getting formatting errors (trying $0 within the string). I want to explore methods to remove this (by using official openai tool call for structure, vs first trying gemini or claude models)
    -> I tried gemini and claude - claude unable to do proper calls. Gemini did amazing and blew me away.
    -> [x] now using gemini and made lots of progress (able to store logs, search logs -- all from ccts generated by cortex as well)

[x ] discovered open source VAD lib based on silero
  -> [x] implementing voice_interface in http://localhost:8000/laboratory/component_viewer
  -> [x] implemented silero 


[x] change status popup to a widget that can be positioned
[x] move audio visualization to center of dashbar 
[x] update html widget (need to trigger automatic re-render, only changing when the full sc
[x] fix background of audio viz (toolbar not responding to dark / light mode and not passing it to  the audio viz




== FIREBASE ==

[ ] Migrate firebase functions to newer version and to parameterized env vars (prevents deploy with missing vars)

== RAI == 
Todo:
- now will shift gears to improving the voice and observability interface

- [ ] observability 
- [ ] transfer local storage to LOGIN (or allow user to toggle) ??
      --> seems it would be great to have a lib that supports a unified interface ? 


- [x] EXPAND THE CONDITIONAL LOGIC TO INCLUDE THREE POSIBILITYs, true/false/missing
- [x] before the template is processed, any text including and after  "@END_TEMPLATE" shouuld be discarded 



Completed:
- [x] legend for the dot phrases that shows next to the voice box
- [x]  NEED TO AUTOFORMAT the text box grammatically (use a fast AI model)
- [x] dot phrase should have Title  and Description  (Description shows first) ] (DONT BREAK
EXIStiNG dot phrases)
- [x] alphabetically sort the dot phrases
- [x] CLick on dot phrase to EDIT it (remove the edit button)
- [x] implement RAI
- [x] upgrade RAI -> persistent state on note generator  | after editing the note - if you leave and come back
- [x] get rid of default templates for good
- [x] enter after dot pharse works too


== BUGS ===

In cortex - html display widget enables injection of javascript into page :( 



Possible upgrades 
-> everytime you interact with cortex it keeps a running summary of the conversation stored into the database (run in background thread) as well as a summary of the graph updates

-> asynchronous tools that can be checked in on
-> various ways of reducing latency
-> initializing the system in a non blocking way -- or with all blocking calls in parallel
-> potentially exploring changing calling methodology for run_call_chain_template (different model?)

-> [x] try with gemini flash model -> wow works incredibly well

HOME AND LOGIN AT THE TOP